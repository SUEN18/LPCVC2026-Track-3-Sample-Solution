{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd92d16-9a94-4e20-a08e-28e08f5669eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pathlib\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Read the notebook config file\n",
    "from aimet_utils.DotDict import DotDict, custom_nb_config\n",
    "with open('config/nb_config_tang.yml', 'r') as f:\n",
    "    nb_cfg = yaml.safe_load(f)\n",
    "nb_cfg = DotDict.from_dict(custom_nb_config(nb_cfg))\n",
    "\n",
    "# Setting NSP Target\n",
    "# Select quantsim config based on target\n",
    "htp_config_file = nb_cfg.model.htp_config_file\n",
    "device = 'cuda'\n",
    "ARN = nb_cfg.model.ARN\n",
    "\n",
    "\n",
    "from utilities.profiler import event_marker\n",
    "\n",
    "from huggingface.baseline_models.qwen2 import modeling_qwen2 as modeling_qwen2\n",
    "from transformers import cache_utils\n",
    "# from aimet_torch.pro.utils.profiler import event_marker\n",
    "from llm_utils.qc_adaptation import (QcAttention, bypass_update_causal_mask, MLP_prepare_conv, ForCausalLM_prepare_conv, MLP_forward_conv, DynamicCache_update,\n",
    "                                     DynamicCache_get_seq_length, update_attr)\n",
    "\n",
    "with event_marker(\"FP model adaptation configuration\"):\n",
    "    modeling_qwen2.QWEN2_ATTENTION_CLASSES['eager'] = QcAttention\n",
    "\n",
    "    # Bypass attention_mask preparation\n",
    "    assert update_attr(modeling_qwen2.Qwen2Model, '_update_causal_mask', bypass_update_causal_mask) or \\\n",
    "           update_attr(modeling_qwen2.Qwen2Model, '_prepare_decoder_attention_mask', bypass_update_causal_mask), \\\n",
    "           f\"neither _prepare_decoder_attention_mask(..) nor _update_causal_mask(..) found, Unknown Qwen2Model definition in {modeling_qwen2.Qwen2Model}\"\n",
    "\n",
    "    # Adaptation to use Conv instead of Linear\n",
    "    setattr(modeling_qwen2.Qwen2MLP, 'prepare_conv', MLP_prepare_conv)\n",
    "    setattr(modeling_qwen2.Qwen2MLP, 'forward_conv', MLP_forward_conv)\n",
    "    setattr(modeling_qwen2.Qwen2ForCausalLM, 'prepare_conv', ForCausalLM_prepare_conv)\n",
    "\n",
    "    # Adapting KVS management\n",
    "    assert update_attr(cache_utils.DynamicCache, 'update', DynamicCache_update), f\"Unknown DynamicCache definition: {cache_utils.DynamicCache}\"\n",
    "    assert update_attr(cache_utils.DynamicCache, 'get_seq_length', DynamicCache_get_seq_length), f\"Unknown DynamicCache definition: {cache_utils.DynamicCache}\"\n",
    "\n",
    "# ---\n",
    "# #### 2.2 Instantiate adapted FP32 model definition\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "cache_dir = nb_cfg.model.cache_dir\n",
    "output_dir = nb_cfg.output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ===========================Configurable setting by users===========================\n",
    "model_id = nb_cfg.model.model_id\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "llm_config = AutoConfig.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)\n",
    "\n",
    "# Setting context length to be 2048 here, user can change this value to ones' desire (but less than Qwen2' trained context length)\n",
    "context_length = nb_cfg.model.context_length\n",
    "# To help with debugging num_hidden_layers could be set to 2 to quickly verify the pipeline and export a two layer model for verification purposes\n",
    "if nb_cfg.profiling.qk_layer:\n",
    "    llm_config.num_hidden_layers = 2\n",
    "\n",
    "print(f'num_layer: {llm_config.num_hidden_layers}, context_length : {context_length},'\n",
    "      f'num_hidden_size :{llm_config.num_attention_heads}, num_kv_heads: {llm_config.num_key_value_heads}')\n",
    "\n",
    "# ===========================Fixed setting that should not be changed by users===========================\n",
    "# Auto-regression length: number of tokens to consume and number of logits to produce.\n",
    "# This value should NOT be changed due to downstream consumption requirements\n",
    "\n",
    "setattr(llm_config, 'return_top_k', 0)\n",
    "setattr(llm_config, 'return_new_key_value_only', True)\n",
    "setattr(llm_config, 'transposed_key_cache', True)\n",
    "setattr(llm_config, 'use_combined_mask_input', True)\n",
    "setattr(llm_config, 'use_position_embedding_input', True)\n",
    "setattr(llm_config, 'use_cache', True)\n",
    "setattr(llm_config, '_attn_implementation', 'eager')\n",
    "setattr(llm_config, '_attn_implementation_internal', 'eager')\n",
    "setattr(llm_config, 'use_input_embeddings', True)\n",
    "setattr(llm_config, 'mask_neg', nb_cfg.model.mask_neg)\n",
    "setattr(llm_config, 'rm_div', True)\n",
    "\n",
    "model_name = os.path.basename(model_id).lower()\n",
    "model_name = model_name.replace(\".\", \"p\").replace(\"-\", \"_\")\n",
    "\n",
    "################################################################################## the above code is copied from llm.ipynb ##################################################################################\n",
    "\n",
    "\n",
    "def slice_inputs_and_run_successive_kvcache_inference_custom(batch_id, fpm, input_ids=None, input_embeds=None, args=None, **kwargs):\n",
    "    if input_ids is not None:\n",
    "        input_length = input_ids.shape[1]\n",
    "    else:\n",
    "        input_length = input_embeds.shape[1]\n",
    "\n",
    "    outputs = {}\n",
    "    attention_mask = kwargs.pop('attention_mask', None)\n",
    "\n",
    "    cnt = 0\n",
    "    for idx in range(0, input_length, fpm.num_tokens)[::-1]:\n",
    "        if cnt >= args.eval_token:\n",
    "            break\n",
    "        \n",
    "        idx = input_length - idx\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            cache_offset = attention_mask.shape[1] - input_length\n",
    "            kwargs[\"attention_mask\"] = attention_mask[:, max(0, cache_offset + idx - fpm.max_tokens):cache_offset + idx]\n",
    "\n",
    "        if input_ids is not None:\n",
    "            cur_outputs = fpm(input_ids=input_ids[:, max(0, idx - fpm.num_tokens):idx], **kwargs)\n",
    "        elif input_embeds is not None:\n",
    "            prepared_inputs, kvcache_info_bundle = fpm.prepare_inputs(input_ids=None, input_embeddings=input_embeds[:, max(0, idx - fpm.num_tokens):idx], **kwargs)\n",
    "            outputs_step = fpm.model(**prepared_inputs)\n",
    "\n",
    "            save_dir = args.save_path\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(prepared_inputs, save_dir + f\"inputs_b{batch_id}_t{cnt}.pt\")\n",
    "            torch.save(outputs_step, save_dir + f\"outputs_b{batch_id}_t{cnt}.pt\")\n",
    "\n",
    "            cur_outputs = fpm.prepare_outputs(outputs_step, prepared_inputs, kvcache_info_bundle)\n",
    "            cnt += 1\n",
    "\n",
    "            # cur_outputs = fpm(input_ids=None, input_embeddings=input_embeds[:, max(0, idx - fpm.num_tokens):idx, :],\n",
    "            #                   **kwargs) ### this is another equivalent method to generate cur_outputs but slower as it calls fpm again\n",
    "        else:\n",
    "            print(\"No input_ids or input_embeds provided to inference generator!\")\n",
    "            assert False\n",
    "\n",
    "        # get valid outputs\n",
    "        bsz, length, dim = cur_outputs['lm_logits'].shape\n",
    "\n",
    "        outputs['lm_logits'] = torch.cat(\n",
    "            (outputs.get('lm_logits', torch.zeros((bsz, 0, dim), device=fpm.device)), cur_outputs['lm_logits']),\n",
    "            dim=1)\n",
    "        kwargs['past_key_values'] = outputs['past_key_values'] = cur_outputs['past_key_values']\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def generate_inout(model_mode, data_loader, forward_pass_manager, num_batches=0, args=None):\n",
    "\n",
    "    if num_batches == 0:\n",
    "        num_batches = len(data_loader)\n",
    "    loss = 0\n",
    "\n",
    "    for batch_id, batch in enumerate(tqdm(data_loader, total=num_batches, desc=\"Evaluating\")):\n",
    "        if batch_id >= num_batches:\n",
    "            break\n",
    "        if model_mode == \"kvcache\":\n",
    "            outputs = slice_inputs_and_run_successive_kvcache_inference_custom(batch_id, forward_pass_manager, input_embeds=batch['input_embedding'])\n",
    "        elif model_mode == \"bertcache\":\n",
    "            outputs = forward_pass_manager(**batch)\n",
    "        # outputs = slice_inputs_and_run_successive_kvcache_inference(forward_pass_manager, input_embeds=batch['inputs_embeds'])\n",
    "\n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--return_dict\", action=\"store_false\", help=\"return dict by default\")\n",
    "    parser.add_argument(\"--eval_batch\", default=10, help='Number of batches generated.', required=False, type=int)\n",
    "    parser.add_argument(\"--eval_token\", default=10, help='Number of tokens generated for each batch.', required=False, type=int)\n",
    "    parser.add_argument(\"--save_path\", default='', help='Dir to save.', required=False, type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with event_marker('FP model'):\n",
    "        model = modeling_qwen2.Qwen2ForCausalLM.from_pretrained(model_id, config=llm_config)\n",
    "        # model.config.return_dict = False\n",
    "        model.config.return_dict = args.return_dict ##### this is set to be true to compare the results with quantized model outputs!!! #####\n",
    "        os.environ['TOKENIZERS_PARALLELISM'] = '0'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, use_fast=True, trust_remote_code=True)\n",
    "        ## Adjust the tokenizer to limit to context_length\n",
    "        tokenizer.model_max_length = context_length\n",
    "\n",
    "    with event_marker('FP model adaptation for NSP backend completion'):\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, \"prepare_conv\"):\n",
    "                module.prepare_conv()\n",
    "\n",
    "    # Loading the calibration data from notebook config\n",
    "    if nb_cfg.calib.name == 'json':\n",
    "        # device = \"cuda:0\"\n",
    "        device = \"cpu\"\n",
    "        from llm_utils.qwen2_vl_dataloader import get_qwen2_dataset\n",
    "        qwen2_dataset_setting = {\n",
    "            \"emb_length\": ARN,\n",
    "            \"device\": device,\n",
    "            \"qwen2vl_model_id\": nb_cfg.model.model_id,\n",
    "            \"calibration_dataset_path\": nb_cfg.calib.calibration_dataset_path,\n",
    "            \"ppl_evaluation_dataset_path\": nb_cfg.calib.ppl_evaluation_dataset_path,\n",
    "            \"image_dataset_path\": nb_cfg.calib.image_dataset_path,\n",
    "            \"vision_input_size\": nb_cfg.calib.vision_input_size\n",
    "        }\n",
    "        train_dataloader, test_dataloader, dataset = get_qwen2_dataset(model.model, qwen2_dataset_setting, num_test_batches=100)\n",
    "\n",
    "    elif nb_cfg.calib.name == 'wiki':\n",
    "        from llm_utils.wikitext_dataloader import get_wiki_dataset\n",
    "        train_dataloader, test_dataloader, _ = get_wiki_dataset(context_length, tokenizer, cache_dir)\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid dataset setting from notebook config\")\n",
    "\n",
    "\n",
    "    # ---\n",
    "    # ### 4. Generate input and output\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    # from llm_utils.forward_pass_wrapper import slice_inputs_and_run_successive_kvcache_inference\n",
    "\n",
    "    # ### 4.1 FP32 PPL Eval\n",
    "    from llm_utils.forward_pass_wrapper import LLMForwardPassManager\n",
    "\n",
    "    orig_fpm = LLMForwardPassManager(cfg=llm_config, model=model, tokenizer=tokenizer,\n",
    "                                     model_mode='kvcache', num_logits_to_return=ARN, separate_tuple_input_output=False,\n",
    "                                     num_tokens=ARN)\n",
    "\n",
    "    with event_marker(\"FP eval\"):\n",
    "        with torch.no_grad():\n",
    "            with orig_fpm.place_on_device(device):\n",
    "                 _ = generate_inout('kvcache', test_dataloader, orig_fpm, num_batches=args.eval_batch, args=args)\n",
    "\n",
    "    print(\"Processing completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
